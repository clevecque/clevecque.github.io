<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8" />
        <title>Le Deep Learning comme outil de création musicale</title>

        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

        <link rel="stylesheet" href="index.css" />
        <script type="text/javascript">
        //<![CDATA[
            $(function() {
                navVisible = false;
                nav = $("#nav");
                nav.hide();
                onScroll = function() {
                    scrollTop = $(window).scrollTop();
                    if (navVisible) {
                        if (scrollTop < 570) {
                            nav.hide();
                            navVisible = false;
                        }
                    }
                    else {
                        if (scrollTop >= 570) {
                            nav.fadeIn(300);
                            navVisible = true;
                        }
                    }
                };
                onScroll();
                $(window).scroll(onScroll);
            });
        //]]>
        </script>

        <link href="https://fonts.googleapis.com/css?family=Oswald:300,400|Lato" rel="stylesheet">
    </head>

    <body data-spy="scroll" data-target="#navspy">

<nav class="navbar navbar-default navbar-fixed-top c-navbar" id="nav">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <a class="navbar-brand" href="#">Le Deep Learning comme outil de création musicale</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="navspy">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="#etat-art">État de l'art</a></li>
        <li><a href="#ia-en-bref">IA en bref</a></li>
        <li><a href="#application">Application</a></li>
        <li><a href="#conclusion">Regard critique</a></li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

        <div class="jumbotron c-main-header">

                <div class="c-cover">            
                    <h1>Le Deep Learning comme outil de création musicale</h1>
                    <h2>Veille technologique — Clémence Lévecque — <a href="https://twitter.com/clemlvcq">@clemlvcq</a></h2>
                </div>

                <span class="copyright"><a href="https://www.youtube.com/watch?v=XUs6CznN8pw">Break Free - Taryn Southern</a></span>
            
        </div>
        
        <div class="container c-intro">
		      <p> Et si vous n'étiez plus capables de reconnaître une création musicale humaine d'une création automatisée ? L'intelligence artificielle menace-t-elle réellement les artistes actuellement ? Petit tour d'horizon de ces termes et ces avancées technologiques qui remodèlent l'industrie musicale. </p>
              <figure>
                <img src="images/music_industry_revenues.png" alt="Chiffre d'affaires lié à l'industrie musicale" width="950" />
                <figcaption>Chiffre d'affaires lié à l'industrie musicale depuis 1999 (<a href="https://www.ifpi.org/downloads/GMR2018.pdf">IFPI</a>).
            </figure>
        </div>
        <div class="container c-content">
            <p>
                La musique et tout ce qui y est relié a toujours été intimiment au progrés humain plus largement, se développant au fil des siècles grâce aux avancées techniques. Cependant, elle a subi un bouleversement avec l'arrivée et la généralisation de la musique assistée par ordinateur.
            </p>
            <p> 
                Depuis, beaucoup a changé: de la commercialisation aux instruments utilisés, l'industrie de la musique a un nouveau visage et beaucoup de nouveaux acteurs. L'Homme continue sa recherche de soi et ses expérimentations grâce aux outils mis à sa disposition. Et logiquement, on entend de plus en plus souvent parlé de projets musicaux qui font intervenir l'intelligence artificielle.
            </p>
            <p> 
                Mais qu'est-ce qui est vraiment désigné par ces termes ? Quelle est la part réellement automatisée de la part humaine ? L'intelligence artificielle est un terme qui peut faire peur ou du moins grincer des dents: on ignore souvent ce qu'il désigne et on sait que ça n'a plus l'air d'être seulement de la science-fiction. Cette nouvelle intelligence pourrait-elle prendre notre place, être plus créative que l'Homme ?
            </p>
        </div>

        <hr />

        <div class="container c-content" id="etat-art">
            <h1>État de l'art</h1>
            <p>
                Commençons cette réflexion quelques avancées en lien avec l'évolution de l'industrie musicale.
                <ul>
                    <li> 1972: Commercialisation de <i><b>Pong</b></i>, le premier jeu vidéo d'arcade. Pas de lien évident avec le sujet abordé ici mais il s'avére que ce jeu a été le premier à inclure du son. Allan Alcorn, designer du jeu, s'est attelé à la tâche de recréer un bruit de rebond pour la balle et des applaudissements à partir de zéros et de uns. <sup><a href="#ref1">[1]</a></sup> Pour les plus curieux ou nostalgiques, ci-dessous un extrait du jeu.
                    </li>
                    <figure>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/fiShX2pTz9A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </figure>
                    <li> Arrivée de la <i><b>MAO</b></i> : abbréviation pour "Musique Assistée par Ordinateur", il s'agit d'un <i>"outil d'aide à la création musicale"</i> <sup><a href="#ref2">[2]</a></sup>. C'est-à-dire d'aide à la composition grâce aux sons présents sur l'ordinateur, le traitement audio, la modification de sons et bien d'autres. Attention, on ne parle cependant pas encore de musique générée par ordinateur, la variable humaine est absolument nécessaire. Ces nouveautés sont possibles grâce à l'arrivée des synthétiseurs entre autres.
                    </li>
                    <li> Années 1980, le protocole <i><b>MIDI</b></i> pour <i>"Musical Instrument Digital Interface"</i>. Quel objectif ? Normaliser les échanges entre le nouveau matériel électronique de musique. On reviendra plus en détail sur ce protocole par la suite mais il suffit de savoir que c'est un ensemble de règles pour désigner des rythmes et des notes.</li>
                    <li> Depuis, les ordinateurs se sont généralisés, ils possèdent tous une carte son et de très nombreux logiciels existent pour les utiliser et composer. Tout comme de nombreux autres champs, l'augmentation de la capacité de calcul des ordinateurs a permis de largement répandre les nouveaux outils de MAO.
                </ul>
            </p>
            <div class="clearfix"></div>

            <div class="col-md-6"><h2>Des liens variés entre IA et musique</h2>
            <h3> <b>La composition </b></h3>
            <p>
                Les projets de composition par IA sont souvent vivement critiqués et les avis en résultant difficilement objectifs. Par conséquent, une équipe menée par Bob Sturm à KTH en Suède a entraîné un réseau de neurones récurrent sur de la musique traditionnelle folk d'Irelande et en a généré plusieurs sons. Enregistrés par des musiciens traditionnels, l'album ainsi réalisé a été proposé au public avec une fausse histoire pour éviter le biais de jugement. Résultat: des critiques très positives et aucun soupçon d'une composition réalisée par un ordinateur. Voyez par vous-mêmes ce qui a été obtenu. <sup><a href="#ref3">[3]</a></sup>
                <figure>
                    <iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/475948881&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe> 
                </figure>
            </p>
            <p>
                D'autres projets ont été ouvertements présentés comme tels:
                    <ul>
                        <li> L'album <i>"I am AI"</i> de Taryn Southern et en particulier la chanson <i>Break Free</i>: la production, c'est-à-dire la musique derrière, a été composée par un outil appelé Amper AI (présenté ci-contre) puisque la chanteuse ne sait jouer d'aucun instrument. <sup><a href="#ref4">[4]</a></sup>
                        </li>
                    </ul>
                    <figure>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/XUs6CznN8pw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </figure>
                    <ul>
                        <li> Un autre projet, servant de campagne publicitaire pour un téléphone, est l'écriture des mouvements III et IV de la <i>"Symphonie Inachevée" (Symphonie n°8)</i> de Schubert par <b>Huawei</b>. Cette fois, l'intelligence artificielle a été entraînée avec le début de la symphonie, d'autres oeuvres de Schubert ainis que des oeuvres d'autres compositeurs, connus pour l'avoir influencé. <sup><a href="#ref5">[5]</a></sup>
                        </li>

                    </ul>
            </p>
            <h3><b> La génération de paroles </b></h3>
            <p>
                David Bowie, artiste mondialement reconnu comme ayant eu une influence immense sur le monde du rock et de la pop, avait comme technique pour écrire ses morceaux de découper des phrases d'articles de journaux, les mettre dans un chapeau puis en tirer au hasard pour créer ses paroles. De cette idée est née le <i><b>Verbasizer</b></i> en 1995, un logiciel développé par Ty Roberts qui permettait de recréer ce processus d'écriture grâce à un ordinateur <sup><a href="#ref6">[6]</a></sup>. Évidemment il n'y a alors pas encore d'intelligence artificielle derrière cette création.
                <figure>
                    <img src="images/verbasizer.jpg" alt="Interface du Verbasizer" width="500" />
                    <figcaption>Interface du Verbasizer (<a href="https://www.nomen.fr/blog/verbasizer">Nomen</a>).</figcaption>
                </figure>
            </p>
            <p>
                Mais dans la continuité de cette idée, on peut présenter le travail plus récent de chercheurs de l'université de Antwerp et l'institut Meertens d'Amsterdam qui ont créé un réseau de neurones pour générer des paroles de rap. Pour rendre le projet, nommé <i><b>Deep Flow</b></i> plus stimulant, ils ont donc aussi créé une petite interface de jeu dont l'objectif est de distinguer des paroles générées de véritables paroles de rap. Le projet est disponible ici : <a href="https://deep-flow.nl/">https://deep-flow.nl/</a> et bien que les paroles générées semblent parfois très réalistes, les chercheurs tempérent en rappelant que les résultats ne sont satisfaisants que sur des textes courts.
            </p>
            <h3><b> La recommandation </b></h3>
            <p>
                Enfin, un dernier pan assez conséquent de l'utilisation d'IA en lien avec la musique est la recommandation de sons pour les utilisateurs. Les plateformes de streaming proposent désormais quasiment toutes ce service de playlist personnalisée selon les artistes préférés, les goûts, la localisation voire parfois même l'heure et le jour. <sup><a href="#ref7">[7]</a></sup>
            </p>
            </div>
            <div class="col-md-6"><h2>Les outils déjà à disposition pour la composition</h2>
                <p>
                    Petit tour non exhaustif d'outils disponibles...
                </p>
                <h3><b> <a href="https://www.ampermusic.com/"> Amper Music</a></b></h3>
                <p>
                    Amper était une start-up qui s'est lancée en 2014 et est parfois aujourd'hui désignée comme "l'entreprise leader en création musicale par IA" <sup><a href="#ref8">[8]</a></sup>. Ce titre lui est en particulier accordé grâce à sa facilité d'utilisation: une personne sans aucune compétence en composition musicale ou en informatique peut très rapidement créer un extrait de 30 secondes assez satisfaisant. L'entreprise insiste sur la qualité du sons et sur le fait qu'ils enregistrent eux-mêmes leurs instruments.
                </p>
                <p>
                    L'interface est très facile d'utilisation puisqu'il suffit de choisir un genre musical, puis une ambiance, le logiciel génère alors un extrait de la longueur souhaitée que l'on peut ensuite retravailler, en choisissant les instruments par exemple.
                </p>
                <figure>
                    <img src="images/amper.jpg" alt="Interface Amper Music" width="500" />
                    <figcaption>Interface de l'outil Amper Music avec à gauche les menus de choix et au centre la timeline musicale (<a href="https://www.ampermusic.com/">Amper</a>). </figcaption>
                </figure>
                <p>
                    Une version Beta était disponible en ligne jusque récemment, il faut désormais acheter un abonnement pour avoir accés au service. 
                </p>
                <p>
                    <b>Particularité du service</b>: outre le point exprimé précédemment sur sa facilité d'utilisation, la particularité d'<i>Amper Music</i> repose sur la construction de son réseau de neurones. Il a été entraîné à <u>reconnaître le lien entre des émotions ou des ambiances et la musique</u> plutôt que sur un enchaînement de notes et de rythmes.
                </p>

                <h3><b> <a href="https://www.ibm.com/case-studies/ibm-watson-beat" >IBM Watson Beat</a></b></h3>
                <p>
                    Watson d'IBM est un programme d'intelligence artificielle créé au départ pour répondre à des questions en langage naturel mais qui depuis s'est étendu à de nombreux autres champs. En particulier Watson Beat est la version de cette IA destinée à composer de la musique.
                </p>
                <p> Il requiert légérement plus de travail puisque cette fois-ci il faut fournir un extrait musical sur lequel le modèle va se baser ainsi qu'un genre. Watson Beat va, grâce à une certaine connaissance en théorie musicale, broder autour de cet extrait et l'enrichir en fonction du genre demandé. Vous pouvez écouter des extraits ci-dessous, à partir de mélodies de l'<i>Ode à la joie</i> ou de <i>Joyeux anniversaire</i>.
                </p>
                <figure>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/Z5ymVzTUU6Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </figure> 
                <p>En sortie, Watson Beat fournit des fichiers MIDI. Il est donc intéressant d'être à l'aise avec ces formats puisqu'il y a un fichier MIDI par ligne (par instruments par exemple). L'avantage de cette solution est que si une des pistes ne convient pas il suffit de la supprimer, voire de la remplacer par une piste faite manuellement.</p>

                <h3><b>Les autres</b></h3>
                <p>
                    Il existe de nombreux autres programmes développés ou en cours de développement mais qui fonctionnent toujours de manière similaire. Ils sont présentés comme étant des outils d'<b>assistance</b> à la création musicale.
                </p>
                <p>
                    Dans chaque cas, l'IA est entraînée avec une très large base de musiques et d'extraits musicaux, qui vont alimenter le modèle de Deep Learning choisi. Selon la variété de ce qui lui a été fourni, elle sera plus ou moins spécialisée dans un domaine. Par exemple, <a href="https://www.aiva.ai/">AIVA</a>, pour "Artificial Intelligence Virtual Artist", a commencé par être entraînée à partir de musique classique occidentale et a été connue pour créer des symphonies comme ci-dessous.
                </p>
                <figure>
                    <iframe width="100%" height="150" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/284587399&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe>
                </figure>
                <p> D'un autre côté, on retrouve par exemple <a href="https://www.flow-machines.com/">Flow Machines</a> à l'origine par <b>Sony CSL</b> qui est un aussi un outil d'aide à la composition, mais qui a été cette fois entraîné sur une très grande base de données de musique et qui a été connue pour créer un morceau "à la manière" des Beatles.

                <figure>
                    <iframe width="100%" height="150" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/283043580&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe>
                </figure>

            </div>

        </div>

        <div class="c-gray">
            <div class="container c-content" id="ia-en-bref">
                <h1>Intelligence artificielle, ou plutôt "Deep Learning" ?</h1>

                <p>
                    Après avoir vu ces différents projets et les résultats associés, on peut donc un peu lire à tout va le terme <i>Intelligence Artificielle</i>. Certains articles en parlent même comme s'il s'agissait d'une personne physique capable de prendre des décisions par elle-même.
                </p>
                <blockquote>
                    <p>“Mais cette petite ritournelle n’est qu’un début. Après plusieurs mois d’apprentissage, Aiva créé près d’une vingtaine d’opus de solo pour piano. Elle compose ensuite sa première pièce symphonique : <a href="https://soundcloud.com/user-95265362/opus-7-for-orchestra?in=user-95265362/sets/genesis">Le Réveil</a>, en sol dièse mineur, qui est diffusée sur le web le 13 juillet.”</p>
                    <small><cite>Novembre 2016, <a href="https://iq.intel.fr/aiva-lia-qui-compose-de-la-musique-classique/">Intel</a></cite></small>
                </blockquote>
                <p>
                    Alors remettons les choses au clair et faisons un petit tour de tout ce vocabulaire.
                </p>

                <h2>Qu'est-ce que le <i>Deep Learning</i> ?</h2>
                <figure>
                    <img src="images/ai_dl_ml.png" alt="Explain link btw AI and ML" width="700" />
                    <figcaption>Lien entre IA, Machine Learning et Deep Learning (<a href="https://medium.com/@Say2neeraj/what-is-the-difference-between-machine-learning-and-deep-learning-5795e4415be9">Medium</a>). </figcaption>
                </figure>
                <p>
                    Il est d'abord important de comprendre les liens entre ces termes qu'on lit très régulièrement. Ce qu'on nomme <i>Intelligence Artificielle</i> est un concept général, la volonté d'entraîner des machines à se comporter comme des humains. Pour cela, on a besoin d'<i>outils</i> qui sont des outils de Machine Learning. Il s'agit de <b>la capacité de comprendre et d'apprendre sans être directement programmé</b>. Un peu comme un enfant qui apprend à lire ou à parler.
                </p>
                <p>
                    Sont désignés comme étant du Machine Learning des algorithmes qui sont en fait des régressions linéaires ou des régressions logistiques (choix binaire). Grâce à une grande quantité de données, on va tenter d'observer des "règles" dans ces données afin de pouvoir à l'avenir prédire un comportement. 
                    <ul>
                        <li>Si on veut prédire le prix d'une maison en fonction de ses caractéristiques, on pourra utiliser une régression linéaire par exemple. </li>
                        <li>Si on veut plutôt savoir si un aliment est comestible ou non en fonction d'une large base de données d'aliments et des conséquences qu'ils ont eu, on utilisera une régression logistique.</li>
                    </ul>
                </p>
                <p>
                    Sur l'image affichée ci-dessus on voit des imbrications entre les termes selon une définition de Andrey Bulezyuk. <sup><a href="#ref9">[9]</a></sup> Le Deep Learning est une sous-branche du Machine Learning, les algorithmes et modèles de Deep Learning sont aussi des algorithmes et modèles de Machine Learning. Ces algorithmes sont très largement inspirés par la structure des neurones présents dans le cerveau humain. 
                </p>
                <p>
                    Finalement, le Deep Learning ou "apprentissage profond" en français correspond à <b>un ensemble de méthodes</b> (souvent des réseaux de neurones, aussi dits profonds) qui utilisent des transformations non linéaires pour représenter des données avec un haut niveau d'abstraction. On utilise en particulier ces méthodes pour de la vision par ordinateur, de la reconnaissance ou génération de texte, ou dans notre cas, de la génération de musique.
                </p>

                <h2>Les différents types d'apprentissage</h2>
                <p>
                    On distingue en Machine Learning plusieurs types d'apprentissage:
                    <ul>
                        <li>
                            <u>L'apprentissage supervisé</u>: on montre à l'algorithme ce qu'on souhaite avoir en sortie grâce à des données étiquetées. Par exemple, si on veut différencier des photos de chiens et de chats, on va utiliser une base de données faite de photos de chiens et de chats, lui dire quelles photos correspondent à quoi puis le laisser apprendre en fonction de ça.
                        </li>
                        <li>
                            <u>L'apprentissage non-supervisé</u>: contrairement au cas précédent, on ne sait pas ce qu'on souhaite avoir en sortie, on laisse l'algorithme trouver une structure ou une logique dans les données. Par exemple, on travaille sur une population d'acheteurs que l'on souhaite découper en catégories significatives mais sans en savoir plus. On va donc laisser l'algorithme faire les catégories qu'il juge les plus cohérentes (ce qu'on appelle du clustering dans ce cas).
                        </li>
                        <li>
                            <u>L'apprentissage semi-supervisé</u>: on mélange un peu les deux cas précédents puisqu'il va prendre en entrée des données annotées et d'autres non.
                        </li>
                        <li>
                            <u>L'apprentissage par renforcement</u>: un peu plus complexe à appréhender, on va travailler en cycle et "récompenser" le programme lors d'une bonne performance. Le but étant qu'il reproduise des "bonnes performances" quand il se retrouve dans des situations similaires.
                        </li>
                    </ul>
                </p>
                <h3>
                    Selon vous, quel est le type d'apprentissage préféré dans notre cas ?
                </h3>
                <p>
                    En fait, beaucoup des projets cités précédemment utilisent de l'apprentissage par renforcement: en se basant sur des notions de théorie musicale, il y a des enchaînements à respecter ou non et on va donc "récompenser" l'algorithme lorsque ces enchaînements sont bien respectés. <sup><a href="#ref10">[10]</a></sup>
                </p>

                <h2>Les réseaux de neurones récurrents</h2>
                <p>
                    Aussi abbrégé en <i>RNN</i> pour "Recurrent Neural Networks", les réseaux de neurones récurrents sont des réseaux de neurones avec une dépendance temporelle entre les noeuds (ou neurones). Dans un réseau de neurones standard, tout est indépendant alors que dans le cas d'un RNN, un nouvel état va dépendre du précédent. Il est donc logique qu'on retrouve régulièrement cett architecture pour de la prédiction de langage (ou de musique ici) puisque chaque nouveau mot, ou nouvelle note, pour avoir une cohérence générale dépend de ce qui a été dit, ou écrit précédemment.
                </p>
                <figure>
                    <img src="images/schema_RNN.png" alt="Schéma évolutif RNN-LSTM" width="700" />
                    <figcaption>Schéma montrant la granularité entre les différentes méthodes </figcaption>
                </figure>
                <p>
                    Enfin on désigne par <i>LSTM</i> pour Long-Short Term Memory, un cas de réseaux de neurones récurrents où chaque unité du réseau va avoir une mémoire ajustable en fonction du travail que l'on souhaite faire. Par exemple, est-ce que l'on préfère ne tenir compte que de l'état précédent ou des trois états précédents.
                </p>
            </div>    
        </div>

        <div class="container c-content" id="application">
            <h1>Application grâce au format MIDI</h1>
            <p>
                Nous allons rapidement aborder, dans cette partie, comment créer un réseau de neurones pour générer de la musique en parlant un peu de théorie musicale et de l'utilisation du format MIDI.
            </p>
            <div class="col-md-6">
                <h2>Théorie musicale</h2>
                <blockquote>
                    <p>"La musique est un exercice caché d'arithmétique, l'esprit n'ayant pas conscience qu'il est en train de compter "</p>
                    <small><cite>1712, Leibniz</cite></small>
                </blockquote>
                <p>
                   Pendant très longtemps, la musique a été considérée comme une science au même titre que la géométrie ou l'astronomie. C'est dès le VI<sup>e</sup> siècle avant Jésus-Christ, que sans connaître encore le terme utilisé aujourd'hui Pythagore théorise les fréquences associées aux notes.
                </p>
                <p>
                    En musique, un <b>intervalle</b> est la distance qui sépare deux notes. S'il y a deux note dans l'intervalle on parle d'une <b>seconde</b>, trois notes forment une <b>tierce</b>, quatre une <b>quarte</b> et ainsi de suite. Tous ces intervalles peuvent être exprimés en fonction de leurs fréquences. Par exemple, si on choisit deux notes dont les fréquences sont <i>f<sub>1</sub></i> pour la plus grave et <i>f<sub>2</sub></i> pour la plus aiguë, si l'intervalle entre ces deux notes un octave (do et do séparés de 7 notes par exemple) alors <i>f<sub>2</sub></i>/<i>f<sub>1</sub></i> = 2. Pour une quinte, le rapport va être de 3/2. 
                </p>
                <figure>
                    <img src="images/gamme_de_bach.gif" alt="Explication de la gamme" width="500" />
                    <figcaption>Description de la gamme selon Bach (<a href="http://wiki.scienceamusante.net/index.php/Les_tiges_musicales"> Wiki Science Amusante</a>)</figcaption>
                </figure>
                <p>
                    Maintenant, construisons la gamme comme on la connaît en Europe, communément appelée <b>gamme tempérée</b> (d'autres gammes existent en Asie par exemple). Les grecs considéraient que seuls la quinte et l'octave étaient <b>consonantes</b>. Par conséquent on va construire toute la gamme à partir de quintes et d'octaves. Choisissons une note de fréquence <i>f</i> (disons qu'il s'agit d'un do), une quinte au dessus on retrouve sol et un octave au-dessus, un autre do de fréquence 2<i>f</i>. La note située une quinte au dessus du sol est un ré de fréquence 9/4<i>f</i>, ce qui est donc au-dessus de 2<i>f</i>, on va donc soustraire une octave et on arrive au ré de fréquence 9/8<i>f</i>. Et on va construire toutes les notes de la gamme de cette façon, seulement à partir de quintes et d'octaves.
                </p>
                <p>
                    En fait, il est impossible de construire une gamme en n'utilisant que des intervalles naturels, autrement dit on ne retombe pas exactement sur la même fréquence à la fin de la boucle. Et à partir de cette impossibilité, il en découle une multitude gammes différentes rien que dans le système occidental. Pour beaucoup plus de détails, sur les liens entre mathématiques et musique puisque celui exposé ici correspond seulement à la base musicale, le travail de Mr. Coulon nommé <a href="http://rcoulon.perso.math.cnrs.fr/papiers/musique.pdf">Maths et musique</a> est extrêmement bien expliqué.
                </p>
                <p>
                    Ce qu'il faut cependant retenir c'est que chaque note a une <b>relation mathématique</b> avec une autre par leur fréquence.
                </p>

            </div>
            <div class="col-md-6">
                <h2>Le format MIDI<sup><a href="#ref11">[11]</a></sup></h2>
                <p>
                    Le format MIDI est en quelque sorte à la musique ce que l'écriture est au discours. C'est un format exclusivement destiné à la musique mais qui ne contient pas de l'audio comme tel. Lorsqu'on enregistre sa voix, on a souvent un fichier MP3 en sortie qu'il est quasiment impossible de modifier. Le format MIDI, qui est d'ailleurs plus un <b>protocole</b>, est un ensemble de règles et nécessite un <b>synthétiseur</b> pour en faire ressortir la musique.
                </p>
                <p>
                    Chaque fichier MIDI est un ensemble de <b>tracks</b> qui contiennent des <b>événements</b>. Les tracks peuvent être différents instruments ou rien que main droite/main gauche sur le piano par exemple. Les événements sont composés de deux informations à chaque fois : une information sur le <b>temps</b> (MIDI time) et une information sur le <b>message</b> (MIDI message).
                        <ul>
                            <li>
                                MIDI time: la plus petite notion de temps est nommée <b>tick</b>. Donc l'information qu'on passe ici est plutôt un delta de temps et désigne combien de ticks on doit attendre avant de jouer le prochain message.
                            </li>
                        </ul>
                        <figure>
                            <img src="images/midi.jpg" alt="Détail découpage temporel MIDI" width="500" />
                            <figcaption>Détails sur le découpage temporel du format MIDI (<a href="https://pdfs.semanticscholar.org/6d50/71756fe4c304981656cb1be9be7f5611ac53.pdf">Hilscher M.</a>)</figcaption>
                        </figure>
                        <ul>
                            <li>
                                MIDI message: le message contient une description de n'importe quelle information pour la partition, appui sur une note ou de levée, le type d'instrument etc
                            </li>
                        </ul>
                </p>
                <h3><b>Message MIDI</b></h3>
                <p>
                    Le message MIDI commence par une commande qui est suivie par de la donnée supplémentaire au besoin. Par conséquent, tous les messages ne font pas la même longueur. La commande la plus utilisée est <i>note-on</i> et <i>note-off</i> qui signalent le début d'une nouvelle note ou sa fin. Ces commandes sont essentielles, d'autant plus quand il s'agit d'une <b>polyphonie</b> (plusieurs notes jouées en même temps contrairement à une monophonie) puisqu'il faut explicitement signalé quelle note s'arrête ou laquelle continue.
                </p>
                <p>
                    Vous pouvez observer à quoi ressemble ce format via ce <a href="https://yellow-ray.de/~moritz/midi_rnn/examples.html"> lien </a>, qui regroupe des extraits de Mozart mis en forme pour un réseau de neurones particulier.
                </p>


            </div>
            <div class="clearfix"></div>
            <div>
                <h2>Exemple d'application avec un réseau de neurones récurrents <sup><a href="#ref12">[12]</a></sup></h2> 
                
                <h3><b>Pré-traitement et entraînement</b></h3>
                <p>
                    Une grande partie du travail pour générer de la musique par un RNN ou pour n'importe quel projet de Machine Learning est le <b>prétraitement des données</b>. En effet, ce qu'on va obtenir va énormément dépendre de l'entrée. Par exemple, si on entraîne un réseau avec uniquement des extraits de musique classique, alors le réseau sera très bon dans ce style mais pas du tout pour générer des extraits de rock.
                </p>
                <p>
                    Par conséquent, on peut par exemple choisir de ne travailler que sur l'enchaînement de notes : on ne tient pas compte du rythme mais on cherche seulement à prédire la prochaine note d'une "phrase musicale" comme on prédirait le prochain mot d'une phrase classique.
                </p>
                <p>
                    Donc on a nos extraits musicaux que l'on sépare en une base dite d'<i>entraînement</i> et une de <i>test</i> qui permettent d'ajuster le modèle. On choisit une longueur <i>l</i> qui correspondra à la longueur des phrases utilisées. Donc il faut imaginer une fenêtre de longueur <i>l</i> qui se déplace le long des extraits pour créer les phrases et le modèle tente de prédire la prochaine note.
                </p>
                <figure>
                    <img src="images/model.png" alt="Ex modèle RNN" width="350" />
                    <figcaption>Exemple d'un modèle RNN avec une fenêtre glissante (<a href="https://pdfs.semanticscholar.org/6d50/71756fe4c304981656cb1be9be7f5611ac53.pdf">Hilscher M.</a>)</figcaption>
                </figure>

                <h3><b>Utilisation</b></h3>
                <p>
                    Disons que notre modèle est entraîné et on souhaite l'utiliser. Alors on va lui fournir une phrase dite "d'échauffement" qui représente les phrases avec lesquelles il était entraîné. C'est là que survient l'intérêt de travailler avec un réseau de neurones récurrents: la note prédite ne dépend pas que de la note précédente mais de tout l'enchaînement de notes dans la phrase. Par conséquent, quand on parle d'échauffement on veut seulement désigner le fait d'annoncer le genre de musique souhaité.
                </p>
            </div>


                
        </div>

        <div class="c-gray">
            <div class="container c-content" id="conclusion">
                <h1>Danger ou opportunité ?</h1>
                <p>
                    Comme dit en introduction, ce sujet fait souvent un peu froid dans le dos: si les ordinateurs deviennent assez puissants pour générer de la musique, quelle limite fixons-nous entre l'Homme et la machine si ce n'est plus la créativité ?
                </p>
                <p>
                    Normalement après cette veille et les résultats exposés, les plus sceptiques devraient être un peu rassurés: ce qu'on appelle intelligences artificielles ne sont pas encore capable de générer de la musique sans la main de l'Homme. 
                    <ul>
                        <li>
                            Les modèles sont longs à construire et nécessitent beaucoup d'ajustements manuels
                        </li>
                        <li>
                            Les données fournies en entrée sont fournies par la main humaine suite à un tri et un pré-traitement
                        </li>
                        <li>
                            Beaucoup de projets qui se vendent comme étant "<i>composés par une IA</i>" ont en effet une part où une intelligence artificielle a participé mais ont souvent nécessité un arrangement humain comme la chanson <i>Daddy's Car</i>, que ce soit pour l'écriture des paroles ou pour le mixage final. 
                        </li>
                    </ul>
                </p>
                <p>
                    De nombreux artistes y voient une énorme opportunité comme par exemple <b>Taryn Southern</b> présentée en première partie qui est seulement chanteuse mais aimerait avoir main mise sur ses productions. Tout comme l'art en général est toujours exploré sous toutes ses formes, la musique a le même comportement. L'arrivée des synthétiseurs avait révolutionné la musique mais n'a jamais remplacé l'Homme, ils ont toujours été seulement un outil pour prolonger la créativité humaine.
                </p>
                <p>
                    On peut donc aussi citer d'autres artistes comme <b>Ash Koosha</b> (<a href='https://pitchfork.com/reviews/albums/ash-koosha-return-0/'>Pitchfork</a>, 6/10 sur son album <i>Return 0</i>) ou le projet <a href="http://dadabots.com/">Dadabots</a> de deux artistes qui créent des nouveaux albums de métal ou math rock grâce à une IA.
                </p>
                <p>
                    Plusieurs personnes spécialisées en musique ou sur le sujet se sont prononcées et ne semblent pas inquiètes...
                </p>
                <blockquote>
                    <p>"Composers are under no threat from AI, if Huawei’s finished Schubert symphony is a guide."</p>
                    <small><cite>Goetz Richter, Professeur Associé - Conservatoire de musique de Sydney, pour <a href="http://theconversation.com/composers-are-under-no-threat-from-ai-if-huaweis-finished-schubert-symphony-is-a-guide-111630"> The conversation</a> </cite></small>
                </blockquote>
                <blockquote>
                    <p>
                        “It’s more of intelligence augmentation. We can facilitate your creative process to cut a lot of the bullshit elements of it."
                    </p>
                    <small><cite>Michael Hobe, co-founder d'Amper pour <a href="https://www.theverge.com/2018/8/31/17777008/artificial-intelligence-taryn-southern-amper-music"> The Verge </a></cite></small>
                </blockquote>
                <p>
                    Il est important de noter que bien que des résultats existent déjà et sont très satisfaisants, les morceaux produits sont souvent courts et se répètent très vite. Enfin...
                </p>
                <blockquote>
                    <p>
                        "This is probably a new thing! Despite the fact that music makers are all about inspiration and forward thinking, the music software industry didn’t evolve as it could have.""
                    </p>
                    <small><cite>Elias Kokkinis, co-founder et CTO de accusonus pour <a href="http://www.avidblogs.com/artificial-intelligence-music-making-jobless-future-ahead-us/"> Avid Blogs</a></cite></small>
                </blockquote>
                <p>
                    Comme il est si bien abordé ci-dessus, il est souvent reproché à la société actuelle de vouloir évoluer trop vite et d'être dans cette sur-consommation. Cependant, en stimulant même la plus petite imagination on se retrouve face à une compétition plus intéressante qui fait parfois sortir des artistes venus de "nul part". 
                </p>
                <h3><b> L'IA dans l'art serait-elle finalement un moyen de redonner du pouvoir et de la créativité à tout un chacun ?</b></h3>
        
            </div>
        </div>

      
        <div class="container c-outro">
            <h1>Conclusion</h1>
            <p>
                Comme il est souvent rappelé aux plus sceptiques d'entre nous, l'Intelligence Artificielle et toutes ses branches associées n'en sont pas au point de remplacer l'Homme mais sont plutôt utilisées pour développer des outils très utiles. La musique et l'art en général va continuer à explorer ces possibilités mais n'oublions pas que l'IA n'a d'intelligence que ce que l'humain lui inculque...
            </p>
            
        </div>     

        <div class="container" id="refs">
            <h1>Références</h1>
            <ul class="list-unstyled">
                <li id="ref1">[1] IGN. Al Alcorn Interview (2008). <a href="https://www.ign.com/articles/2008/03/11/al-alcorn-interview">https://www.ign.com/articles/2008/03/11/al-alcorn-interview .</a></li>
                <li id="ref2">[2] Wikipédia. MAO <a href="https://fr.wikipedia.org/wiki/Musique_assist%C3%A9e_par_ordinateur">https://fr.wikipedia.org/wiki/Musique_assist%C3%A9e_par_ordinateur </a></li>
                <li id="ref3">[3] Inc. These Computer-Generated Tunes Were Sent to Real Music Critics and Nobody Noticed the Difference (2018) <a href="https://www.inc.com/eric-mack/these-tunes-were-written-by-artificial-intelligence-can-you-hear-difference.html">https://www.inc.com/eric-mack/these-tunes-were-written-by-artificial-intelligence-can-you-hear-difference.html</a>
                <li id="ref4">[4] Culturebox. Composer grâce à l'intelligence artificielle (2019) <a href="https://culturebox.francetvinfo.fr/musique/composer-grace-a-l-intelligence-artificielle-taryn-southern-l-a-fait-avec-amper-286524">https://culturebox.francetvinfo.fr/musique/composer-grace-a-l-intelligence-artificielle-taryn-southern-l-a-fait-avec-amper-286524 </a>
                <li id="ref5">[5] Digital Trends. Huawei's AI has finished Schubert's Unfinished Symphony. (2019) <a href="https://www.digitaltrends.com/mobile/huawei-ai-unfinished-symphony/">https://www.digitaltrends.com/mobile/huawei-ai-unfinished-symphony/</a>
                <li id="ref6">[6] Motherboard. The Verbasizer was David Bowie's 1995 Lyric-Writing Mac App. (2016) <a href="https://motherboard.vice.com/en_us/article/xygxpn/the-verbasizer-was-david-bowies-1995-lyric-writing-mac-app">https://motherboard.vice.com/en_us/article/xygxpn/the-verbasizer-was-david-bowies-1995-lyric-writing-mac-app</a></li>
                <li id="ref7">[7] Siècle digital. Spotify: des recommandations personnalisées de chansons bientôt dans les playlists officielles ? (2018) <a href='https://siecledigital.fr/2018/09/25/spotify-recommandations-personnalisees-playlist/'>https://siecledigital.fr/2018/09/25/spotify-recommandations-personnalisees-playlist/</a>
                <li id="ref8">[8] Global News Wire. Amper (2019) <a href="https://www.globenewswire.com/news-release/2019/01/23/1704214/0/en/Amper-Music-Launches-First-AI-Music-Composition-Platform-for-Enterprise-Content-Creators.html">https://www.globenewswire.com/news-release/2019/01/23/1704214/0/en/Amper-Music-Launches-First-AI-Music-Composition-Platform-for-Enterprise-Content-Creators.html</a>
                <li id="ref9">[9] BoTree, Differences between Machine Learning et Deep Learning <a href="https://mc.ai/decoding-the-differences-between-machine-learning-deep-learning/">https://mc.ai/decoding-the-differences-between-machine-learning-deep-learning/</a>
                <li id="ref10">[10] Anna Chaney, The Watson Beat (2018) <a href="https://medium.com/@anna_seg/the-watson-beat-d7497406a202"> https://medium.com/@anna_seg/the-watson-beat-d7497406a202</a>
                <li id="ref11">[11] H. M. de Oliveira, R. C. de Oliveira, Understanding MIDI: A painless tutorial on midi format <a href="https://www.researchgate.net/publication/316955785_Understanding_MIDI_A_Painless_Tutorial_on_Midi_Format">https://www.researchgate.net/publication/316955785_Understanding_MIDI_A_Painless_Tutorial_on_Midi_Format</a>
                <li id="ref12">[12] Hilscher M., Shahroudi N., Music Generation for MIDI datasets <a href="https://pdfs.semanticscholar.org/6d50/71756fe4c304981656cb1be9be7f5611ac53.pdf">https://pdfs.semanticscholar.org/6d50/71756fe4c304981656cb1be9be7f5611ac53.pdf</a>

        </div>
    </body>
</html>
`
