<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8" />
        <title>Le Deep Learning comme outil de création musicale</title>

        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

        <link rel="stylesheet" href="index.css" />
        <script type="text/javascript">
        //<![CDATA[
            $(function() {
                navVisible = false;
                nav = $("#nav");
                nav.hide();
                onScroll = function() {
                    scrollTop = $(window).scrollTop();
                    if (navVisible) {
                        if (scrollTop < 570) {
                            nav.hide();
                            navVisible = false;
                        }
                    }
                    else {
                        if (scrollTop >= 570) {
                            nav.fadeIn(300);
                            navVisible = true;
                        }
                    }
                };
                onScroll();
                $(window).scroll(onScroll);
            });
        //]]>
        </script>

        <link href="https://fonts.googleapis.com/css?family=Oswald:300,400|Lato" rel="stylesheet">
    </head>

    <body data-spy="scroll" data-target="#navspy">

<nav class="navbar navbar-default navbar-fixed-top c-navbar" id="nav">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <a class="navbar-brand" href="#">Le Deep Learning comme outil de création musicale</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="navspy">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="#etat-art">État de l'art</a></li>
        <li><a href="#ia-en-bref">IA en bref</a></li>
        <li><a href="#theorie-musicale">Théorie musicale</a></li>
        <li><a href="#conclusion">Regard critique</a></li>
      </ul>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

        <div class="jumbotron c-main-header">

                <div class="c-cover">            
                    <h1>Le Deep Learning comme outil de création musicale</h1>
                    <h2>Veille technologique — Clémence Lévecque — <a href="https://twitter.com/clemlvcq">@clemlvcq</a></h2>
                </div>

                <span class="copyright"><a href="https://www.youtube.com/watch?v=XUs6CznN8pw">Break Free - Taryn Southern</a></span>
            
        </div>
        
        <div class="container c-intro">
		      <p> Et si vous n'étiez plus capables de reconnaître une création musicale humaine d'une création automatisée ? L'intelligence artificielle menace-t-elle réellement les artistes actuellement ? Petit tour d'horizon de ces termes et ces avancées technologiques qui remodèlent l'industrie musicale. </p>
              <figure>
                <img src="images/music_industry_revenues.png" alt="Chiffre d'affaires lié à l'industrie musicale" width="950" />
                <figcaption>Chiffre d'affaires lié à l'industrie musicale depuis 1999 (<a href="https://www.ifpi.org/downloads/GMR2018.pdf">IFPI</a>).
            </figure>
        </div>
        <div class="container c-content">
            <p>
                La musique et tout ce qui y est relié a toujours été intimiment au progrés humain plus largement, se développant au fil des siècles grâce aux avancées techniques. Cependant, elle a subi un bouleversement avec l'arrivée et la généralisation de la musique assistée par ordinateur.
            </p>
            <p> 
                Depuis, beaucoup a changé: de la commercialisation aux instruments utilisés, l'industrie de la musique a un nouveau visage et beaucoup de nouveaux acteurs. L'Homme continue sa recherche de soi et ses expérimentations grâce aux outils mis à sa disposition. Et logiquement, on entend de plus en plus souvent parlé de projets musicaux qui font intervenir l'intelligence artificielle.
            </p>
            <p> 
                Mais qu'est-ce qui est vraiment désigné par ces termes ? Quelle est la part réellement automatisée de la part humaine ? L'intelligence artificielle est un terme qui peut faire peur ou du moins grincer des dents: on ignore souvent ce qu'il désigne et on sait que ça n'a plus l'air d'être seulement de la science-fiction. Cette nouvelle intelligence pourrait-elle prendre notre place, être plus créative que l'Homme ?
            </p>
        </div>

        <hr />

        <div class="container c-content" id="etat-art">
            <h1>État de l'art</h1>
            <p>
                Commençons cette réflexion quelques avancées en lien avec l'évolution de l'industrie musicale.
                <ul>
                    <li> 1972: Commercialisation de <i><b>Pong</b></i>, le premier jeu vidéo d'arcade. Pas de lien évident avec le sujet abordé ici mais il s'avére que ce jeu a été le premier à inclure du son. Allan Alcorn, designer du jeu, s'est attelé à la tâche de recréer un bruit de rebond pour la balle et des applaudissements à partir de zéros et de uns. <sup><a href="#ref1">[1]</a></sup> Pour les plus curieux ou nostalgiques, ci-dessous un extrait du jeu.
                    </li>
                    <figure>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/fiShX2pTz9A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </figure>
                    <li> Arrivée de la <i><b>MAO</b></i> : abbréviation pour "Musique Assistée par Ordinateur", il s'agit d'un <i>"outil d'aide à la création musicale"</i> <sup><a href="#ref2">[2]</a></sup>. C'est-à-dire d'aide à la composition grâce aux sons présents sur l'ordinateur, le traitement audio, la modification de sons et bien d'autres. Attention, on ne parle cependant pas encore de musique générée par ordinateur, la variable humaine est absolument nécessaire. Ces nouveautés sont possibles grâce à l'arrivée des synthétiseurs entre autres.
                    </li>
                    <li> Années 1980, le protocole <i><b>MIDI</b></i> pour <i>"Musical Instrument Digital Interface"</i>. Quel objectif ? Normaliser les échanges entre le nouveau matériel électronique de musique. On reviendra plus en détail sur ce protocole par la suite mais il suffit de savoir que c'est un ensemble de règles pour désigner des rythmes et des notes.</li>
                    <li> Depuis, les ordinateurs se sont généralisés, ils possèdent tous une carte son et de très nombreux logiciels existent pour les utiliser et composer. Tout comme de nombreux autres champs, l'augmentation de la capacité de calcul des ordinateurs a permis de largement répandre les nouveaux outils de MAO.
                </ul>
            </p>
            <div class="clearfix"></div>

            <div class="col-md-6"><h2>Des liens variés entre IA et musique</h2>
            <h3> <b>La composition </b></h3>
            <p>
                Les projets de composition par IA sont souvent vivement critiqués et les avis en résultant difficilement objectifs. Par conséquent, une équipe menée par Bob Sturm à KTH en Suède a entraîné un réseau de neurones récurrent sur de la musique traditionnelle folk d'Irelande et en a généré plusieurs sons. Enregistrés par des musiciens traditionnels, l'album ainsi réalisé a été proposé au public avec une fausse histoire pour éviter le biais de jugement. Résultat: des critiques très positives et aucun soupçon d'une composition réalisée par un ordinateur. Voyez par vous-mêmes ce qui a été obtenu. <sup><a href="#ref3">[3]</a></sup>
                <figure>
                    <iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/475948881&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe> 
                </figure>
            </p>
            <p>
                D'autres projets ont été ouvertements présentés comme tels:
                    <ul>
                        <li> L'album <i>"I am AI"</i> de Taryn Southern et en particulier la chanson <i>Break Free</i>: la production, c'est-à-dire la musique derrière, a été composée par un outil appelé Amper AI (présenté ci-contre) puisque la chanteuse ne sait jouer d'aucun instrument. <sup><a href="#ref4">[4]</a></sup>
                        </li>
                    </ul>
                    <figure>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/XUs6CznN8pw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </figure>
                    <ul>
                        <li> Un autre projet, servant de campagne publicitaire pour un téléphone, est l'écriture des mouvements III et IV de la <i>"Symphonie Inachevée" (Symphonie n°8)</i> de Schubert par <b>Huawei</b>. Cette fois, l'intelligence artificielle a été entraînée avec le début de la symphonie, d'autres oeuvres de Schubert ainis que des oeuvres d'autres compositeurs, connus pour l'avoir influencé. <sup><a href="#ref5">[5]</a></sup>
                        </li>

                    </ul>
            </p>
            <h3><b> La génération de paroles </b></h3>
            <p>
                David Bowie, artiste mondialement reconnu comme ayant eu une influence immense sur le monde du rock et de la pop, avait comme technique pour écrire ses morceaux de découper des phrases d'articles de journaux, les mettre dans un chapeau puis en tirer au hasard pour créer ses paroles. De cette idée est née le <i><b>Verbasizer</b></i> en 1995, un logiciel développé par Ty Roberts qui permettait de recréer ce processus d'écriture grâce à un ordinateur <sup><a href="#ref6">[6]</a></sup>. Évidemment il n'y a alors pas encore d'intelligence artificielle derrière cette création.
                <figure>
                    <img src="images/verbasizer.jpg" alt="Interface du Verbasizer" width="500" />
                    <figcaption>Interface du Verbasizer (<a href="https://www.nomen.fr/blog/verbasizer">Nomen</a>).</figcaption>
                </figure>
            </p>
            <p>
                Mais dans la continuité de cette idée, on peut présenter le travail plus récent de chercheurs de l'université de Antwerp et l'institut Meertens d'Amsterdam qui ont créé un réseau de neurones pour générer des paroles de rap. Pour rendre le projet, nommé <i><b>Deep Flow</b></i> plus stimulant, ils ont donc aussi créé une petite interface de jeu dont l'objectif est de distinguer des paroles générées de véritables paroles de rap. Le projet est disponible ici : <a href="https://deep-flow.nl/">https://deep-flow.nl/</a> et bien que les paroles générées semblent parfois très réalistes, les chercheurs tempérent en rappelant que les résultats ne sont satisfaisants que sur des textes courts.
            </p>
            <h3><b> La recommandation </b></h3>
            <p>
                Enfin, un dernier pan assez conséquent de l'utilisation d'IA en lien avec la musique est la recommandation de sons pour les utilisateurs. Les plateformes de streaming proposent désormais quasiment toutes ce service de playlist personnalisée selon les artistes préférés, les goûts, la localisation voire parfois même l'heure et le jour. <sup><a href="#ref7">[7]</a></sup>
            </p>
            </div>
            <div class="col-md-6"><h2>Les outils déjà à disposition pour la composition</h2>
                <p>
                    Petit tour non exhaustif d'outils disponibles...
                </p>
                <h3><b> <a href="https://www.ampermusic.com/"> Amper Music</a></b></h3>
                <p>
                    Amper était une start-up qui s'est lancée en 2014 et est parfois aujourd'hui désignée comme "l'entreprise leader en création musicale par IA" <sup><a href="#ref8">[8]</a></sup>. Ce titre lui est en particulier accordé grâce à sa facilité d'utilisation: une personne sans aucune compétence en composition musicale ou en informatique peut très rapidement créer un extrait de 30 secondes assez satisfaisant. L'entreprise insiste sur la qualité du sons et sur le fait qu'ils enregistrent eux-mêmes leurs instruments.
                </p>
                <p>
                    L'interface est très facile d'utilisation puisqu'il suffit de choisir un genre musical, puis une ambiance, le logiciel génère alors un extrait de la longueur souhaitée que l'on peut ensuite retravailler, en choisissant les instruments par exemple.
                </p>
                <figure>
                    <img src="images/amper.jpg" alt="Interface Amper Music" width="500" />
                    <figcaption>Interface de l'outil Amper Music avec à gauche les menus de choix et au centre la timeline musicale (<a href="https://www.ampermusic.com/">Amper</a>). </figcaption>
                </figure>
                <p>
                    Une version Beta était disponible en ligne jusque récemment, il faut désormais acheter un abonnement pour avoir accés au service. 
                </p>
                <p>
                    <b>Particularité du service</b>: outre le point exprimé précédemment sur sa facilité d'utilisation, la particularité d'<i>Amper Music</i> repose sur la construction de son réseau de neurones. Il a été entraîné à <u>reconnaître le lien entre des émotions ou des ambiances et la musique</u> plutôt que sur un enchaînement de notes et de rythmes.
                </p>

                <h3><b> <a href="https://www.ibm.com/case-studies/ibm-watson-beat" >IBM Watson Beat</a></b></h3>
                <p>
                    Watson d'IBM est un programme d'intelligence artificielle créé au départ pour répondre à des questions en langage naturel mais qui depuis s'est étendu à de nombreux autres champs. En particulier Watson Beat est la version de cette IA destinée à composer de la musique.
                </p>
                <p> Il requiert légérement plus de travail puisque cette fois-ci il faut fournir un extrait musical sur lequel le modèle va se baser ainsi qu'un genre. Watson Beat va, grâce à une certaine connaissance en théorie musicale, broder autour de cet extrait et l'enrichir en fonction du genre demandé. Vous pouvez écouter des extraits ci-dessous, à partir de mélodies de l'<i>Ode à la joie</i> ou de <i>Joyeux anniversaire</i>.
                </p>
                <figure>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/Z5ymVzTUU6Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </figure> 
                <p>En sortie, Watson Beat fournit des fichiers MIDI. Il est donc intéressant d'être à l'aise avec ces formats puisqu'il y a un fichier MIDI par ligne (par instruments par exemple). L'avantage de cette solution est que si une des pistes ne convient pas il suffit de la supprimer, voire de la remplacer par une piste faite manuellement.</p>

                <h3><b>Les autres</b></h3>
                <p>
                    Il existe de nombreux autres programmes développés ou en cours de développement mais qui fonctionnent toujours de manière similaire. Ils sont présentés comme étant des outils d'<b>assistance</b> à la création musicale.
                </p>
                <p>
                    Dans chaque cas, l'IA est entraînée avec une très large base de musiques et d'extraits musicaux, qui vont alimenter le modèle de Deep Learning choisi. Selon la variété de ce qui lui a été fourni, elle sera plus ou moins spécialisée dans un domaine. Par exemple, <a href="https://www.aiva.ai/">AIVA</a>, pour "Artificial Intelligence Virtual Artist", a commencé par être entraînée à partir de musique classique occidentale et a été connue pour créer des symphonies comme ci-dessous.
                </p>
                <figure>
                    <iframe width="100%" height="150" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/284587399&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe>
                </figure>
                <p> D'un autre côté, on retrouve par exemple <a href="https://www.flow-machines.com/">Flow Machines</a> à l'origine par <b>Sony CSL</b> qui est un aussi un outil d'aide à la composition, mais qui a été cette fois entraîné sur une très grande base de données de musique et qui a été connue pour créer un morceau "à la manière" des Beatles.

                <figure>
                    <iframe width="100%" height="150" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/283043580&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe>
                </figure>

            </div>

        </div>

        <div class="c-gray">
            <div class="container c-content" id="ia-en-bref">
                <h1>Intelligence artificielle, ou plutôt "Deep Learning" ?</h1>

                <p>
                    Après avoir vu ces différents projets et les résultats associés, on peut donc un peu lire à tout va le terme <i>Intelligence Artificielle</i>. Certains articles en parlent même comme s'il s'agissait d'une personne physique capable de prendre des décisions par elle-même.
                </p>
                <blockquote>
                    <p>“Mais cette petite ritournelle n’est qu’un début. Après plusieurs mois d’apprentissage, Aiva créé près d’une vingtaine d’opus de solo pour piano. Elle compose ensuite sa première pièce symphonique : <a href="https://soundcloud.com/user-95265362/opus-7-for-orchestra?in=user-95265362/sets/genesis">Le Réveil</a>, en sol dièse mineur, qui est diffusée sur le web le 13 juillet.”</p>
                    <small><cite>Novembre 2016, <a href="https://iq.intel.fr/aiva-lia-qui-compose-de-la-musique-classique/">Intel</a></cite></small>
                </blockquote>
                <p>
                    Alors remettons les choses au clair et faisons un petit tour de tout ce vocabulaire.
                </p>

                <h2>Qu'est-ce que le <i>Deep Learning</i> ?</h2>
                <figure>
                    <img src="images/ai_dl_ml.png" alt="Explain link btw AI and ML" width="700" />
                    <figcaption>Lien entre IA, Machine Learning et Deep Learning (<a href="https://medium.com/@Say2neeraj/what-is-the-difference-between-machine-learning-and-deep-learning-5795e4415be9">Medium</a>). </figcaption>
                </figure>
                <p>
                    Il est d'abord important de comprendre les liens entre ces termes qu'on lit très régulièrement. Ce qu'on nomme <i>Intelligence Artificielle</i> est un concept général, la volonté d'entraîner des machines à se comporter comme des humains. Pour cela, on a besoin d'<i>outils</i> qui sont des outils de Machine Learning. Il s'agit de <b>la capacité de comprendre et d'apprendre sans être directement programmé</b>. Un peu comme un enfant qui apprend à lire ou à parler.
                </p>
                <p>
                    Sont désignés comme étant du Machine Learning des algorithmes qui sont en fait des régressions linéaires ou des régressions logistiques (choix binaire). Grâce à une grande quantité de données, on va tenter d'observer des "règles" dans ces données afin de pouvoir à l'avenir prédire un comportement. 
                    <ul>
                        <li>Si on veut prédire le prix d'une maison en fonction de ses caractéristiques, on pourra utiliser une régression linéaire par exemple. </li>
                        <li>Si on veut plutôt savoir si un aliment est comestible ou non en fonction d'une large base de données d'aliments et des conséquences qu'ils ont eu, on utilisera une régression logistique.</li>
                    </ul>
                </p>
                <p>
                    Sur l'image affichée ci-dessus on voit des imbrications entre les termes selon une définition de Andrey Bulezyuk. <sup><a href="#ref9">[9]</a></sup> Le Deep Learning est une sous-branche du Machine Learning, les algorithmes et modèles de Deep Learning sont aussi des algorithmes et modèles de Machine Learning. Ces algorithmes sont très largement inspirés par la structure des neurones présents dans le cerveau humain. 
                </p>
                <p>
                    Finalement, le Deep Learning ou "apprentissage profond" en français correspond à <b>un ensemble de méthodes</b> (souvent des réseaux de neurones, aussi dits profonds) qui utilisent des transformations non linéaires pour représenter des données avec un haut niveau d'abstraction. On utilise en particulier ces méthodes pour de la vision par ordinateur, de la reconnaissance ou génération de texte, ou dans notre cas, de la génération de musique.
                </p>

                <h2>Les différents types d'apprentissage</h2>
                <p>
                    On distingue en Machine Learning plusieurs types d'apprentissage:
                    <ul>
                        <li>
                            <u>L'apprentissage supervisé</u>: on montre à l'algorithme ce qu'on souhaite avoir en sortie grâce à des données étiquetées. Par exemple, si on veut différencier des photos de chiens et de chats, on va utiliser une base de données faite de photos de chiens et de chats, lui dire quelles photos correspondent à quoi puis le laisser apprendre en fonction de ça.
                        </li>
                        <li>
                            <u>L'apprentissage non-supervisé</u>: contrairement au cas précédent, on ne sait pas ce qu'on souhaite avoir en sortie, on laisse l'algorithme trouver une structure ou une logique dans les données. Par exemple, on travaille sur une population d'acheteurs que l'on souhaite découper en catégories significatives mais sans en savoir plus. On va donc laisser l'algorithme faire les catégories qu'il juge les plus cohérentes (ce qu'on appelle du clustering dans ce cas).
                        </li>
                        <li>
                            <u>L'apprentissage semi-supervisé</u>: on mélange un peu les deux cas précédents puisqu'il va prendre en entrée des données annotées et d'autres non.
                        </li>
                        <li>
                            <u>L'apprentissage par renforcement</u>: un peu plus complexe à appréhender, on va travailler en cycle et "récompenser" le programme lors d'une bonne performance. Le but étant qu'il reproduise des "bonnes performances" quand il se retrouve dans des situations similaires.
                        </li>
                    </ul>
                </p>
                <h3>
                    Selon vous, quel est le type d'apprentissage préféré dans notre cas ?
                </h3>
                <p>
                    En fait, beaucoup des projets cités précédemment utilisent de l'apprentissage par renforcement: en se basant sur des notions de théorie musicale, il y a des enchaînements à respecter ou non et on va donc "récompenser" l'algorithme lorsque ces enchaînements sont bien respectés. <sup><a href="#ref10">[10]</a></sup>
                </p>

                <h2>Les réseaux de neurones récurrents</h2>
                <p>
                    Aussi abbrégé en <i>RNN</i> pour "Recurrent Neural Networks", les réseaux de neurones récurrents sont des réseaux de neurones avec une dépendance temporelle entre les noeuds (ou neurones). Dans un réseau de neurones standard, tout est indépendant alors que dans le cas d'un RNN, un nouvel état va dépendre du précédent. Il est donc logique qu'on retrouve régulièrement cett architecture pour de la prédiction de langage (ou de musique ici) puisque chaque nouveau mot, ou nouvelle note, pour avoir une cohérence générale dépend de ce qui a été dit, ou écrit précédemment.
                </p>
                <figure>
                    <img src="images/schema_RNN.png" alt="Schéma évolutif RNN-LSTM" width="700" />
                    <figcaption>Schéma montrant la granularité entre les différentes méthodes </figcaption>
                </figure>
                <p>
                    Enfin on désigne par <i>LSTM</i> pour Long-Short Term Memory, un cas de réseaux de neurones récurrents où chaque unité du réseau va avoir une mémoire ajustable en fonction du travail que l'on souhaite faire. Par exemple, est-ce que l'on préfère ne tenir compte que de l'état précédent ou des trois états précédents.
                </p>
            </div>    
        </div>

        <div class="container c-content" id="theorie-musicale">
            <h1>Théorie musicale et mathématisation</h1>
            <p>
                Nous allons rapidement aborder, dans cette partie, de la théorie musicale et en particulier de l'utilisation du format MIDI dans le cas de génération de musique.
            </p>
            <div class="col-md-6">
                <h2>Théorie musicale</h2>
                <blockquote>
                    <p>"La musique est un exercice caché d'arithmétique, l'esprit n'ayant pas conscience qu'il est en train de compter "</p>
                    <small><cite>1712, Leibniz</cite></small>
                </blockquote>
                <p>
                   Pendant très longtemps, la musique a été considérée comme une science au même titre que la géométrie ou l'astronomie. C'est dès le VI<sup>e</sup> siècle avant Jésus-Christ, que sans connaître encore le terme utilisé aujourd'hui Pythagore théorise les fréquences associées aux notes.
                </p>
                <p>
                    En musique, un <b>intervalle</b> est la distance qui sépare deux notes. S'il y a deux note dans l'intervalle on parle d'une <b>seconde</b>, trois notes forment une <b>tierce</b>, quatre une <b>quarte</b> et ainsi de suite. Tous ces intervalles peuvent être exprimés en fonction de leurs fréquences. Par exemple, si on choisit deux notes dont les fréquences sont <i>f<sub>1</sub></i> pour la plus grave et <i>f<sub>2</sub></i> pour la plus aiguë, si l'intervalle entre ces deux notes un octave (do et do séparés de 7 notes par exemple) alors <i>f<sub>2</sub></i>/<i>f<sub>1</sub></i> = 2. Pour une quinte, le rapport va être de 3/2. 
                </p>
                <figure>
                    <img src="images/gamme_de_bach.gif" alt="Explication de la gamme" width="500" />
                    <figcaption>Description de la gamme selon Bach (<a href="http://wiki.scienceamusante.net/index.php/Les_tiges_musicales"> Wiki Science Amusante</a>)</figcaption>
                </figure>
                <p>
                    Maintenant, construisons la gamme comme on la connaît en Europe, communément appelée <b>gamme tempérée</b> (d'autres gammes existent en Asie par exemple). Les grecs considéraient que seuls la quinte et l'octave étaient <b>consonantes</b>. Par conséquent on va construire toute la gamme à partir de quintes et d'octaves. Choisissons une note de fréquence <i>f</i> (disons qu'il s'agit d'un do), une quinte au dessus on retrouve sol et un octave au-dessus, un autre do de fréquence 2<i>f</i>. La note située une quinte au dessus du sol est un ré de fréquence 9/4<i>f</i>, ce qui est donc au-dessus de 2<i>f</i>, on va donc soustraire une octave et on arrive au ré de fréquence 9/8<i>f</i>. Et on va construire toutes les notes de la gamme de cette façon, seulement à partir de quintes et d'octaves.
                </p>
                <p>
                    En fait, il est impossible de construire une gamme en n'utilisant que des intervalles naturels, autrement dit on ne retombe pas exactement sur la même fréquence à la fin de la boucle. Et à partir de cette impossibilité, il en découle une multitude gammes différentes rien que dans le système occidental. Pour beaucoup plus de détails, sur les liens entre mathématiques et musique puisque celui exposé ici correspond seulement à la base musicale, le travail de Mr. Coulon nommé <a href="http://rcoulon.perso.math.cnrs.fr/papiers/musique.pdf">Maths et musique</a> est extrêmement bien expliqué.
                </p>
                <p>
                    Ce qu'il faut cependant retenir c'est que chaque note a une <b>relation mathématique</b> avec une autre par leur fréquence.
                </p>

            </div>
            <div class="col-md-6">
                <h2>Le format MIDI</h2>
                <p>
                    Le format MIDI est en quelque sorte à la musique ce que l'écriture est au discours. C'est un format exclusivement destiné à la musique mais qui ne contient pas de l'audio comme tel. Lorsqu'on enregistre sa voix, on a souvent un fichier MP3 en sortie qu'il est quasiment impossible de modifier. Le format MIDI, qui est d'ailleurs plus un <b>protocole</b>, est un ensemble de règles et nécessite un <b>synthétiseur</b> pour en faire ressortir la musique.
                </p>
                <p>
                    Chaque fichier MIDI est un ensemble de <b>tracks</b> qui contiennent des <b>événements</b>. Les tracks peuvent être différents instruments ou rien que main droite/main gauche sur le piano par exemple. Les événements sont des descriptions d'appui sur une note ou de levée, de tempo, du type d'instrument etc soit n'importe quelle information d'intérêt pour la partition. À chaque événement est relié un numéro de <b>tick</b> qui permet de positionner l'événement dans le temps puisqu'il s'agit de la plus petite résolution temporelle.
                </p>
                <figure>
                    <img src="images/midi.jpg" alt="Détail découpage temporel MIDI" width="500" />
                    <figcaption>Détails sur le découpage temporel du format MIDI(<a href="https://pdfs.semanticscholar.org/6d50/71756fe4c304981656cb1be9be7f5611ac53.pdf"> M. Hilscher</a>)</figcaption>
                </figure>

            </div>

                
        </div>

        <div class="c-gray">
            <div class="container c-content" id="conclusion">
                <h1>Danger ou opportunité ?</h1>

            </div>
        </div>

      
        <div class="container c-outro">
            <h1>Conclusion</h1>
            
        </div>     

        <!--
        <div class="c-gray">
            <div class="container c-content">
                <p>Hello</p>
            </div> 
        </div>
        -->
        <div class="container" id="refs">
            <div class="col-md-8">
            <h1>Références</h1>
            <ul class="list-unstyled">
                <li id="ref1">[1] IGN. Al Alcorn Interview (2008). <a href="https://www.ign.com/articles/2008/03/11/al-alcorn-interview">https://www.ign.com/articles/2008/03/11/al-alcorn-interview .</a></li>
                <li id="ref2">[2] Wikipédia. MAO <a href="https://fr.wikipedia.org/wiki/Musique_assist%C3%A9e_par_ordinateur">https://fr.wikipedia.org/wiki/Musique_assist%C3%A9e_par_ordinateur </a></li>
                <li id="ref3">[3] Inc. These Computer-Generated Tunes Were Sent to Real Music Critics and Nobody Noticed the Difference (2018) <a href="https://www.inc.com/eric-mack/these-tunes-were-written-by-artificial-intelligence-can-you-hear-difference.html">https://www.inc.com/eric-mack/these-tunes-were-written-by-artificial-intelligence-can-you-hear-difference.html</a>
                <li id="ref4">[4] Culturebox. Composer grâce à l'intelligence artificielle (2019) <a href="https://culturebox.francetvinfo.fr/musique/composer-grace-a-l-intelligence-artificielle-taryn-southern-l-a-fait-avec-amper-286524">https://culturebox.francetvinfo.fr/musique/composer-grace-a-l-intelligence-artificielle-taryn-southern-l-a-fait-avec-amper-286524 </a>
                <li id="ref5">[5] Digital Trends. Huawei's AI has finished Schubert's Unfinished Symphony. (2019) <a href="https://www.digitaltrends.com/mobile/huawei-ai-unfinished-symphony/">https://www.digitaltrends.com/mobile/huawei-ai-unfinished-symphony/</a>
                <li id="ref6">[6] Motherboard. The Verbasizer was David Bowie's 1995 Lyric-Writing Mac App. (2016) <a href="https://motherboard.vice.com/en_us/article/xygxpn/the-verbasizer-was-david-bowies-1995-lyric-writing-mac-app">https://motherboard.vice.com/en_us/article/xygxpn/the-verbasizer-was-david-bowies-1995-lyric-writing-mac-app</a></li>
                <li id="ref7">[7] Siècle digital. Spotify: des recommandations personnalisées de chansons bientôt dans les playlists officielles ? (2018) <a href='https://siecledigital.fr/2018/09/25/spotify-recommandations-personnalisees-playlist/'>https://siecledigital.fr/2018/09/25/spotify-recommandations-personnalisees-playlist/</a>
                <li id="ref8">[8] Global News Wire. Amper (2019) <a href="https://www.globenewswire.com/news-release/2019/01/23/1704214/0/en/Amper-Music-Launches-First-AI-Music-Composition-Platform-for-Enterprise-Content-Creators.html">https://www.globenewswire.com/news-release/2019/01/23/1704214/0/en/Amper-Music-Launches-First-AI-Music-Composition-Platform-for-Enterprise-Content-Creators.html</a>
                <li id="ref9">[9] BoTree, Differences between Machine Learning et Deep Learning <a href="https://mc.ai/decoding-the-differences-between-machine-learning-deep-learning/">https://mc.ai/decoding-the-differences-between-machine-learning-deep-learning/</a>
                <li id="ref10">[10] Anna Chaney, The Watson Beat (2018) <a href="https://medium.com/@anna_seg/the-watson-beat-d7497406a202"> https://medium.com/@anna_seg/the-watson-beat-d7497406a202</a>

            </div>
        </div>
    </body>
</html>
`
